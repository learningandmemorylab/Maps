{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for computing maximum a-posterior probability estimates of natural\n",
    "parameters given the observed data.\n",
    "To the original code new gradient descent algorithms are added as conjugate\n",
    "gradient and BFGS.\n",
    "---\n",
    "This code implements approximate inference methods for State-Space Analysis of\n",
    "Spike Correlations (Shimazaki et al. PLoS Comp Bio 2012). It is an extension of\n",
    "the existing code from repository <https://github.com/tomxsharp/ssll> (For\n",
    "Matlab Code refer to <http://github.com/shimazaki/dynamic_corr>). We\n",
    "acknowledge Thomas Sharp for providing the code for exact inference.\n",
    "In this library are additional methods provided to perform the State-Space\n",
    "Analysis approximately. This includes pseudolikelihood, TAP, and Bethe\n",
    "approximations. For details see: <http://arxiv.org/abs/1607.08840>\n",
    "Copyright (C) 2016\n",
    "Authors of the extensions: Christian Donner (christian.donner@bccn-berlin.de)\n",
    "                           Hideaki Shimazaki (shimazaki@brain.riken.jp)\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "import numpy\n",
    "\n",
    "import import_ipynb\n",
    "import probability\n",
    "import transforms\n",
    "import pseudo_likelihood\n",
    "\n",
    "\n",
    "\n",
    "# Named function pointers to MAP estimators\n",
    "# SEE BOTTOM OF FILE\n",
    "\n",
    "# Parameters for gradient-ascent methods of MAP estimation\n",
    "MAX_GA_ITERATIONS = 500\n",
    "GA_CONVERGENCE = 1e-4\n",
    "\n",
    "def run(emd, t):\n",
    "    \"\"\"\n",
    "    Computes the MAP estimate of the natural parameters at some timestep, given\n",
    "    the observed spike patterns at that timestep and the one-step-prediction\n",
    "    mean and covariance for the same timestep. This function pass the variables\n",
    "    at time t to the user-specified gradient ascent alogirhtm.\n",
    "    \"\"\"\n",
    "    # Set time bin in pseudo_likelihood\n",
    "    pseudo_likelihood.time_bin = t\n",
    "    # Extract observed patterns and one-step predictions for time t\n",
    "    y_t = emd.y[t,:]\n",
    "    # Data at time t\n",
    "    X_t = emd.spikes[t,:,:]\n",
    "    # Number of runs\n",
    "    R = emd.R\n",
    "    # Initial values of natural parameters\n",
    "    theta_0 = emd.theta_s[t,:]\n",
    "    # Mean and covariance of one-step prediction density\n",
    "    theta_o = emd.theta_o[t,:]\n",
    "    sigma_o = emd.sigma_o[t]\n",
    "    sigma_o_i = emd.sigma_o_inv[t]\n",
    "    # Run the user-specified gradient ascent algorithm\n",
    "    theta_f, sigma_f = emd.max_posterior(y_t, X_t, R, theta_0, theta_o,\n",
    "                                          sigma_o, sigma_o_i, emd.param_est_eta)\n",
    "\n",
    "    return theta_f, sigma_f\n",
    "\n",
    "\n",
    "def newton_raphson(y_t, X_t, R, theta_0, theta_o, sigma_o, sigma_o_i, *args):\n",
    "    \"\"\"\n",
    "    TODO update comments to elaborate on how this method differs from the others\n",
    "    :param container.EMData emd:\n",
    "        All data pertaining to the EM algorithm.\n",
    "    :param int t:\n",
    "        Timestep for which to compute the maximum posterior probability.\n",
    "    :returns:\n",
    "        Tuple containing the mean and covariance of the posterior probability\n",
    "        density, each as a numpy.ndarray.\n",
    "    \"\"\"\n",
    "    # Initialise the loop guards\n",
    "    max_dlpo = numpy.inf\n",
    "    iterations = 0\n",
    "    # Initialise theta_max to the smooth theta value of the previous iteration\n",
    "    theta_max = theta_0\n",
    "    # Iterate the gradient ascent algorithm until convergence or failure\n",
    "    while max_dlpo > GA_CONVERGENCE:\n",
    "        # Compute the eta of the current theta values\n",
    "        p = transforms.compute_p(theta_max)\n",
    "        eta = transforms.compute_eta(p)\n",
    "        # Compute the first derivative of the posterior prob. w.r.t. theta_max\n",
    "        dllk = R * (y_t - eta)\n",
    "        dlpr = -numpy.dot(sigma_o_i, theta_max - theta_o)\n",
    "        dlpo = dllk + dlpr\n",
    "        # Compute the second derivative of the posterior prob. w.r.t. theta_max\n",
    "        ddlpo = -R * transforms.compute_fisher_info(p, eta) - sigma_o_i\n",
    "        # Dot the results to climb the gradient, and accumulate the\n",
    "        # Small regularization added to avoid singular matrices\n",
    "        ddlpo_i = numpy.linalg.inv(ddlpo + numpy.finfo(float).eps*\\\n",
    "                                   numpy.identity(eta.shape[0]))\n",
    "        # Update Theta\n",
    "        theta_max -= numpy.dot(ddlpo_i, dlpo)\n",
    "        # Update the look guard\n",
    "        max_dlpo = numpy.amax(numpy.absolute(dlpo)) / R\n",
    "        # Count iterations\n",
    "        iterations += 1\n",
    "        # Check for check for overrun\n",
    "        if iterations == MAX_GA_ITERATIONS:\n",
    "            raise Exception('The maximum-a-posterior gradient-ascent '+\\\n",
    "                'algorithm did not converge before reaching the maximum '+\\\n",
    "                'number iterations.')\n",
    "\n",
    "    return theta_max, -ddlpo_i\n",
    "\n",
    "\n",
    "def conjugate_gradient(y_t, X_t, R, theta_0, theta_o, sigma_o, sigma_o_i, *args):\n",
    "    \"\"\" Fits with `Nonlinear Conjugate Gradient Method\n",
    "    <https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method>`_.\n",
    "    :param container.EMData emd:\n",
    "        All data pertaining to the EM algorithm.\n",
    "    :param int t:\n",
    "        Timestep for which to compute the maximum posterior probability.\n",
    "    :returns:\n",
    "        Tuple containing the mean and covariance of the posterior probability\n",
    "        density, each as a numpy.ndarray.\n",
    "    @author: Christian Donner\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize theta with previous smoothed theta\n",
    "    theta_max = theta_0\n",
    "    # Get p and eta values for current theta\n",
    "    p = transforms.compute_p(theta_max)\n",
    "    eta = transforms.compute_eta(p)\n",
    "    # Compute derivative of posterior\n",
    "    dllk = R*(y_t - eta)\n",
    "    dlpr = -numpy.dot(sigma_o_i, theta_max - theta_o)\n",
    "    dlpo = dllk + dlpr\n",
    "    # Initialize stopping criterion variables\n",
    "    max_dlpo = 1.\n",
    "    iterations = 0\n",
    "    # Get theta gradient\n",
    "    d_th = dlpo\n",
    "    # Set initial search direction\n",
    "    s = dlpo\n",
    "    # Compute line search\n",
    "    theta_max, dlpo, p, eta = line_search(theta_max, y_t, R, p, s, dlpo,\n",
    "                                          theta_o, sigma_o_i)\n",
    "\n",
    "    # Iterate until convergence or failure\n",
    "    while max_dlpo > GA_CONVERGENCE:\n",
    "\n",
    "        # Set current theta gradient to previous\n",
    "        d_th_prev = d_th\n",
    "        # The new theta gradient\n",
    "        d_th = dlpo\n",
    "        # Calculate beta\n",
    "        beta = compute_beta(d_th, d_th_prev)\n",
    "        # New search direction\n",
    "        s = d_th + beta * s\n",
    "        # Line search\n",
    "        theta_max, dlpo, p, eta = line_search(theta_max, y_t, R, p, s, dlpo,\n",
    "                                              theta_o, sigma_o_i)\n",
    "        # Get maximal entry of log posterior grad divided by number of trials\n",
    "        max_dlpo = numpy.amax(numpy.absolute(dlpo)) / R\n",
    "        # Count iterations\n",
    "        iterations += 1\n",
    "        if iterations == MAX_GA_ITERATIONS:\n",
    "            raise Exception('The maximum-a-posterior conjugate-gradient '+\\\n",
    "                'algorithm did not converge before reaching the maximum '+\\\n",
    "                'number iterations.')\n",
    "\n",
    "    # Compute final covariance matrix\n",
    "    ddllk = - R*transforms.compute_fisher_info(p, eta)\n",
    "    ddlpo = ddllk - sigma_o_i\n",
    "    ddlpo_i = numpy.linalg.inv(ddlpo)\n",
    "\n",
    "    return theta_max, -ddlpo_i\n",
    "\n",
    "\n",
    "def bfgs(y_t, X_t, R, theta_0, theta_o, sigma_o, sigma_o_i, *args):\n",
    "    \"\"\" Fits due to `Broyden-Fletcher-Goldfarb-Shanno algorithm\n",
    "    <https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%\n",
    "    80%93Shanno_algorithm>`_.\n",
    "    :param container.EMData emd:\n",
    "        All data pertaining to the EM algorithm.\n",
    "    :param int t:\n",
    "        Timestep for which to compute the maximum posterior probability.\n",
    "    :returns:\n",
    "        Tuple containing the mean and covariance of the posterior probability\n",
    "        density, each as a numpy.ndarray.\n",
    "    @author: Christian Donner\n",
    "    \"\"\"\n",
    "\n",
    "    # # Initialize theta with previous smoothed theta\n",
    "    theta_max = theta_0\n",
    "    # Get p and eta values for current theta\n",
    "    p = transforms.compute_p(theta_max)\n",
    "    eta = transforms.compute_eta(p)\n",
    "    # Initialize the estimate of the inverse fisher info\n",
    "    ddlpo_i_e = numpy.identity(theta_max.shape[0])\n",
    "    # Compute derivative of posterior\n",
    "    dllk = R*(y_t - eta)\n",
    "    dlpr = -numpy.dot(sigma_o_i, theta_max - theta_o)\n",
    "    dlpo = dllk + dlpr\n",
    "    # Initialize stopping criterion variables\n",
    "    max_dlpo = 1.\n",
    "    iterations = 0\n",
    "\n",
    "    # Iterate until convergence or failure\n",
    "    while max_dlpo > GA_CONVERGENCE:\n",
    "        # Compute direction for line search\n",
    "        s_dir = numpy.dot(dlpo, ddlpo_i_e)\n",
    "        # Set theta to old theta\n",
    "        theta_prev = numpy.copy(theta_max)\n",
    "        # Set current log posterior gradient to previous\n",
    "        dlpo_prev = dlpo\n",
    "        # Perform line search\n",
    "        theta_max, dlpo, p, eta = line_search(theta_max, y_t, R, p, s_dir, dlpo,\n",
    "                                              theta_o, sigma_o_i)\n",
    "        # Get the difference between old and new theta\n",
    "        d_theta = theta_max - theta_prev\n",
    "        # Difference in log posterior gradients\n",
    "        dlpo_diff = dlpo_prev - dlpo\n",
    "        # Project gradient change on theta change\n",
    "        dlpo_diff_dth = numpy.inner(dlpo_diff, d_theta)\n",
    "        # Compute estimate of covariance matrix with Sherman-Morrison Formula\n",
    "        a = (dlpo_diff_dth + \\\n",
    "             numpy.dot(dlpo_diff, numpy.dot(ddlpo_i_e, dlpo_diff.T)))*\\\n",
    "            numpy.outer(d_theta, d_theta)\n",
    "        b = numpy.inner(d_theta, dlpo_diff)**2\n",
    "        c = numpy.dot(ddlpo_i_e, numpy.outer(dlpo_diff, d_theta)) + \\\n",
    "            numpy.outer(d_theta, numpy.inner(dlpo_diff, ddlpo_i_e))\n",
    "        d = dlpo_diff_dth\n",
    "        ddlpo_i_e += (a/b - c/d)\n",
    "        # Get maximal entry of log posterior grad divided by number of trials\n",
    "        max_dlpo = numpy.amax(numpy.absolute(dlpo)) / R\n",
    "        # Count iterations\n",
    "        iterations += 1\n",
    "        if iterations == MAX_GA_ITERATIONS:\n",
    "            raise Exception('The maximum-a-posterior bfgs-gradient '+\\\n",
    "                'algorithm did not converge before reaching the maximum '+\\\n",
    "                'number iterations.')\n",
    "\n",
    "    # Compute final covariance matrix\n",
    "    ddllk = -R*transforms.compute_fisher_info(p, eta)\n",
    "    ddlpo = ddllk - sigma_o_i\n",
    "    ddlpo_i = numpy.linalg.inv(ddlpo)\n",
    "\n",
    "    return theta_max, -ddlpo_i\n",
    "\n",
    "\n",
    "def line_search(theta_max, y, R, p, s, dlpo, theta_o, sigma_o_i):\n",
    "    \"\"\" Searches the minimum on a line with quadratic approximation\n",
    "    :param numpy.ndarray theta_max:\n",
    "        Starting point on the line\n",
    "    :param numpy.ndarray y:\n",
    "        Empirical mean of the data (sufficient statistics)\n",
    "    :param int R:\n",
    "        Number of trials\n",
    "    :param numpy.ndarray p:\n",
    "        Probability for each pattern\n",
    "    :param numpy.ndarray s:\n",
    "        Direction that is searched in\n",
    "    :param numpy.ndarray dlpo:\n",
    "        Derivative of of th posterior at the current theta\n",
    "    :param numpy.ndarray theta_o:\n",
    "        One-step prediction of theta\n",
    "    :param numpy.ndarray sigma_o_i:\n",
    "        One-step prediction of the covariance matrix\n",
    "    :returns\n",
    "        Tuple containing the minimum on the line, the log posterior gradient,\n",
    "        the current p and current eta vector\n",
    "    This method approximates at each point the log posterior quadratically\n",
    "    and searches iteratively for the minimum.\n",
    "    @author: Christian Donner\n",
    "    \"\"\"\n",
    "    y_s = numpy.dot(y, s)\n",
    "    # Project theta on p_map\n",
    "    theta_p = transforms.p_map.dot(theta_max)\n",
    "    # Project p-map on search direction\n",
    "    p_map_s = transforms.p_map.dot(s)\n",
    "    # Projected eta on search direction\n",
    "    eta_s = numpy.dot(p_map_s, p)\n",
    "    # Project inverse one-step covariance matrix on search direction\n",
    "    sigma_o_i_s = numpy.dot(sigma_o_i, s)\n",
    "    # Project gradient of log posterior on search direction\n",
    "    dlpo_s = numpy.dot(dlpo, s)\n",
    "    # Get Metric of fisher info along s direction\n",
    "    s_G_s = R*(numpy.dot(p_map_s, p*p_map_s) - eta_s**2) + \\\n",
    "            numpy.dot(s, sigma_o_i_s)\n",
    "    # Initialize iteration variable and alpha\n",
    "    dalpha = numpy.inf\n",
    "    alpha = 0\n",
    "    snorm = numpy.sum(numpy.absolute(s))\n",
    "    while dalpha*snorm > 1e-2:\n",
    "        # Compute alpha due to gradient\n",
    "        alpha_new = alpha + dlpo_s/s_G_s\n",
    "        # If new alpha is negative take the half of old alpha\n",
    "        if alpha_new < 0:\n",
    "            alpha /= 2.\n",
    "            dalpha = alpha\n",
    "        # Else take new\n",
    "        else:\n",
    "            dalpha = numpy.absolute(alpha - alpha_new)\n",
    "            alpha = alpha_new\n",
    "        # Update theta\n",
    "        theta_tmp = theta_max + alpha*s\n",
    "        # Compute new psi\n",
    "        psi_new = numpy.log(numpy.sum(numpy.exp(theta_p + alpha*p_map_s)))\n",
    "        # psi = numpy.log(p*numpy.exp(alpha*p_map_s))\n",
    "        p = numpy.exp(theta_p + alpha*p_map_s - psi_new)\n",
    "        # Project eta on search direction\n",
    "        eta_s = numpy.dot(p_map_s, p)\n",
    "        # Project fisher information on search direction\n",
    "        s_G_s = R*(numpy.dot(p_map_s, p*p_map_s) - eta_s**2) + \\\n",
    "                numpy.dot(s, sigma_o_i_s)\n",
    "        # Compute log posterior gradient projected on s\n",
    "        dllk_s = R*(y_s - eta_s)\n",
    "        dlpr_s = -numpy.dot(sigma_o_i_s, theta_tmp - theta_o)\n",
    "        dlpo_s = dllk_s + dlpr_s\n",
    "    # return optimized theta and current gradient of log posterior\n",
    "\n",
    "    eta = transforms.compute_eta(p)\n",
    "    dllk = R*(y - eta)\n",
    "    dlpr = -numpy.dot(sigma_o_i, theta_tmp - theta_o)\n",
    "    dlpo = dllk + dlpr\n",
    "    return theta_tmp, dlpo, p, eta\n",
    "\n",
    "\n",
    "def compute_beta(df, dfp, s=None, which='PR'):\n",
    "    \"\"\" Computes the beta Polak Ribiere Formula\n",
    "    :param numpy.ndarray df:\n",
    "        gradient of function to minimize\n",
    "    :param numpy.ndarray dfp:\n",
    "        previous gradient of function to minimize\n",
    "    :returns float:\n",
    "        result of Polak Ribiere Formula\n",
    "    @author: Christian Donner\n",
    "    \"\"\"\n",
    "\n",
    "    # Polak Ribiere Formula\n",
    "    if which == 'PR':\n",
    "        beta = float(numpy.dot(df, (df - dfp)) / numpy.dot(dfp, dfp))\n",
    "    elif which == 'HS':\n",
    "        if numpy.allclose(df, dfp):\n",
    "            return 0\n",
    "        beta = -float(numpy.dot(df, (df - dfp)) / numpy.dot(s, (df - dfp)))\n",
    "    return numpy.amax([0, beta])\n",
    "\n",
    "\n",
    "# Named function pointers to MAP estimators\n",
    "functions = {'nr': newton_raphson,\n",
    "             'cg': conjugate_gradient,\n",
    "             'bf': bfgs}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
