{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for generating synthetic spike data.\n",
    "Major change: Adding Gibbs sampler for spike trains.\n",
    "---\n",
    "This code implements approximate inference methods for State-Space Analysis of\n",
    "Spike Correlations (Shimazaki et al. PLoS Comp Bio 2012). It is an extension of\n",
    "the existing code from repository <https://github.com/tomxsharp/ssll> (For\n",
    "Matlab Code refer to <http://github.com/shimazaki/dynamic_corr>). We\n",
    "acknowledge Thomas Sharp for providing the code for exact inference.\n",
    "In this library are additional methods provided to perform the State-Space\n",
    "Analysis approximately. This includes pseudolikelihood, TAP, and Bethe\n",
    "approximations. For details see: <http://arxiv.org/abs/1607.08840>\n",
    "Copyright (C) 2016\n",
    "Authors of the extensions: Christian Donner (christian.donner@bccn-berlin.de)\n",
    "                           Hideaki Shimazaki (shimazaki@brain.riken.jp)\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "import numpy\n",
    "import random\n",
    "import pdb\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import import_ipynb\n",
    "import transforms\n",
    "\n",
    "\n",
    "def generate_thetas(N, O, T, mu1=-2., sigma1=50, mu2=0., sigma2=50, alpha=12., ratio_modulated=1.):\n",
    "    \"\"\" Generates dynamic thetas by a Gaussian Process.\n",
    "    :param int N:\n",
    "        Number of cells\n",
    "    :param int O:\n",
    "        Order of model\n",
    "    :param int T:\n",
    "        Number of time bins\n",
    "    :param numpy.float mu1:\n",
    "        Mean of first order parameters for the Gaussian process (Default=-2)\n",
    "    :param float sigma1:\n",
    "        Determines how strong first order parameters change over time (Default=50)\n",
    "    :param float mu2:\n",
    "        Mean of couplings for the Gaussian process (Default=0)\n",
    "    :param float sigma2:\n",
    "        Determines how strong couplings change over time (Default=50)\n",
    "    :param float alpha:\n",
    "        Scalar that determines the correlations over time of the Gaussian process is (Default=12)\n",
    "    :param float ratio_modulated:\n",
    "        Scalar between 0 and 1, how many of the interaction should be modulated. (Default=1)\n",
    "    :return:\n",
    "        Matrix with dimensions (t, d) with theta parameters generated by GP.\n",
    "         'd' is the dimensionality of the model\n",
    "    \"\"\"\n",
    "    D = transforms.compute_D(N, O)\n",
    "    MU = numpy.tile(mu1, (T, D))\n",
    "    MU[:,N:] = mu2\n",
    "    # Create covariance matrix\n",
    "    X = numpy.tile(numpy.arange(T),(T,1))\n",
    "    K1 = 1./alpha*numpy.exp(-(X - X.transpose()) ** 2 / (2. * sigma1 ** 2))\n",
    "    K2 = 1./alpha*numpy.exp(-(X - X.transpose()) ** 2 / (2. * sigma2 ** 2))\n",
    "    # Generate Gaussian processes\n",
    "    L1 = numpy.linalg.cholesky(K1 + 1e-13 * numpy.eye(T))\n",
    "    L2 = numpy.linalg.cholesky(K2 + 1e-13 * numpy.eye(T))\n",
    "    theta = numpy.empty([T, D])\n",
    "    theta[:,:N] = mu1 + numpy.dot(L1, numpy.random.randn(T, N))\n",
    "    theta[:,N:] = mu2 + numpy.dot(L2, numpy.random.randn(T, D - N))\n",
    "    num_non_modulated = int(numpy.around((1. - ratio_modulated)*(D - N)))\n",
    "    non_modulated_idx = random.sample(range(N,D), num_non_modulated)\n",
    "    theta[:, non_modulated_idx] = 0.\n",
    "    return theta\n",
    "\n",
    "\n",
    "def generate_stationary_thetas(N, O, T):\n",
    "    \"\"\" Generates stationary thetas.\n",
    "    :param int N:\n",
    "        Number of cells\n",
    "    :param int O:\n",
    "        Order of model\n",
    "    :param int T:\n",
    "        Number of time bins\n",
    "    :return:\n",
    "        Array (t, d) with non changing thetas. 'd' is the dimensionality of the model.\n",
    "    \"\"\"\n",
    "    th1, th2 = -3., 0.\n",
    "    D = transforms.compute_D(N, O)\n",
    "    th = numpy.zeros([T,D])\n",
    "    th[:,:N] = th1\n",
    "    th[:,N:] = th2/N*(1 + 0.5*numpy.random.randn(T,D-N))\n",
    "    idx = numpy.triu_indices(N,1)\n",
    "    theta_array = numpy.zeros([N,N])\n",
    "    theta_array[idx[0],idx[1]] = th[0,N:]\n",
    "    theta_array[idx[1],idx[0]] = th[0,N:]\n",
    "    mean_thetas = numpy.mean(theta_array, axis=0)\n",
    "    theta_array -= numpy.tile(mean_thetas, [N, 1])\n",
    "    th[:,N:] = theta_array[idx[0],idx[1]]\n",
    "    return th\n",
    "\n",
    "\n",
    "def generate_spikes(p, R, seed=None):\n",
    "    \"\"\"\n",
    "    Draws spike patterns for each of `R' trial runs from the probability mass\n",
    "    specified in `p'. `p' must have T rows, one independent probability mass for\n",
    "    each timestep, and 2^C columns, where C is the number of cells (maximum of\n",
    "    8) involved in the spike pattern.\n",
    "    :param numpy.ndarray p:\n",
    "        Probability mass of spike patterns for each timestep.\n",
    "    :param int R:\n",
    "        Number of spike patterns to generate for each timestep.\n",
    "    :returns:\n",
    "        Binary matrix with dimensions (time, runs, cells), in which a `1' in\n",
    "        location (t, r, c) denotes a spike at time t in run r by cell c, as a\n",
    "        numpy.ndarray\n",
    "    \"\"\"\n",
    "    # Set metadata\n",
    "    T, N = p.shape[0], numpy.int(numpy.log2(p.shape[1]))\n",
    "    # Initialise random seed\n",
    "    numpy.random.seed(seed)\n",
    "    # Set up spike patterns\n",
    "    fx = transforms.enumerate_patterns(N)\n",
    "    # Set up the output array (time, trials, cells)\n",
    "    T, C = p.shape[0], numpy.log2(p.shape[1])\n",
    "    spikes = numpy.zeros((T, R, N))\n",
    "    # Iterate over each probability\n",
    "    for i in range(T):\n",
    "        # Draw random values from the probability distribution\n",
    "        idx = random_weighted(p[i], R)\n",
    "        # Extract spike patterns for each trial\n",
    "        spikes[i,:,:] = fx[idx,:]\n",
    "\n",
    "    return spikes\n",
    "\n",
    "\n",
    "def random_weighted(p, R):\n",
    "    \"\"\"\n",
    "    Draws `R' integers from the probability mass over the integers `p'.\n",
    "    :param numpy.ndarray p:\n",
    "        Probability mass.\n",
    "    :param int R:\n",
    "        Sample size to draw from `p'.\n",
    "    :returns:\n",
    "        `R' random numbers drawn from distribution `p', as a numpy.ndarray.\n",
    "    \"\"\"\n",
    "    # Take a cumulative sum of the probability mass\n",
    "    cs = numpy.cumsum(p)\n",
    "    # Draw uniform random numbers for each timestep or trial\n",
    "    rnd = numpy.random.random(R)\n",
    "    # For each random value, find the index of the first weight above it\n",
    "    idx = numpy.zeros(R, dtype=numpy.int)\n",
    "    for i in range(R):\n",
    "        idx[i] = numpy.sum(cs < rnd[i])\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def generate_spikes_gibbs(theta, N, O, R, **kwargs):\n",
    "    \"\"\"Generates spike trains for the model given the thetas with\n",
    "    `Gibbs-Sampling.\n",
    "    :param numpy.ndarray theta:\n",
    "        parameters used for sampling for each time bin\n",
    "    :param int N:\n",
    "        Number of cells\n",
    "    :param int O:\n",
    "        Order of interaction\n",
    "    :param int R:\n",
    "        Number of runs that are generated.\n",
    "    :returns:\n",
    "        Binary matrix with dimensions (time, runs, cells), in which a `1' in\n",
    "        location (t, r, c) denotes a spike at time t in run r by cell c, as a\n",
    "        numpy.ndarray\n",
    "    \"\"\"\n",
    "    # Set numpy seed (should be removed at some point)\n",
    "    seed = kwargs.get('seed', None)\n",
    "    numpy.random.seed(seed)\n",
    "    # Set pre-trials\n",
    "    pre_R = kwargs.get('pre_n', 100)\n",
    "    steps = kwargs.get('sample_steps', 1)\n",
    "    # Get number of bins\n",
    "    T = theta.shape[0]\n",
    "    # Initialize array for spike data\n",
    "    X = numpy.zeros([T, R*steps+pre_R, N], dtype=numpy.uint8)\n",
    "    # Gets subsets\n",
    "    subsets = transforms.enumerate_subsets(N, O)\n",
    "    # Get number of natural parameters\n",
    "    D = len(subsets)\n",
    "    # Initialize subset map\n",
    "    subset_map = numpy.zeros([D, N])\n",
    "    # Get map of relevant patterns (d,c)\n",
    "    for i in range(len(subsets)):\n",
    "        subset_map[i, subsets[i]] = 1\n",
    "    # Count how many cells must be active for each theta\n",
    "    subset_count = numpy.sum(subset_map, axis=1)\n",
    "    # Draw random numbers from uniform distribution\n",
    "    rand_numbers = numpy.random.rand(T, steps*R+pre_R, N)\n",
    "    # Iterate over all time bins\n",
    "    for t in range(T):\n",
    "        # Iterate through all Runs\n",
    "        cur_theta = theta[t]\n",
    "        for l in range(1, steps*R+pre_R):\n",
    "            # Iterate through all cells\n",
    "            for i in range(N):\n",
    "                # Construct pattern from trial before and\n",
    "                # from neurons that have been seen in this trial\n",
    "                pattern = numpy.hstack([X[t, l, :i], X[t, l-1, i:]])\n",
    "                # set x^(i,t) to \"1\" and compute f(X) for those\n",
    "                pattern[i] = 1\n",
    "                fx1 = (numpy.dot(pattern, subset_map.T) == subset_count)\n",
    "                # Set x^(i,t) to \"0\" and compute f(X) for those\n",
    "                pattern[i] = 0\n",
    "                fx0 = (numpy.dot(pattern, subset_map.T) == subset_count)\n",
    "                # compute p( x^(i,l) = 1 || X^(1:i-1,t),X^(i+1:N,l-1) )\n",
    "                prob_spike = 0.5*(1 + numpy.tanh(0.5*(numpy.dot(cur_theta,fx1)\n",
    "                                                - numpy.dot(cur_theta,fx0))))\n",
    "                # if smaller than probability X^(i,l) -> 1\n",
    "                X[t, l, i] = numpy.greater_equal(prob_spike,\n",
    "                                                 rand_numbers[t, l, i])\n",
    "    # Return spike data\n",
    "    return X[:, pre_R::steps, :]\n",
    "\n",
    "\n",
    "def generate_spikes_gibbs_parallel(theta, N, O, R, **kwargs):\n",
    "    \"\"\"Generates spike trains for the model given the thetas with\n",
    "    `Gibbs-Sampling <https://en.wikipedia.org/wiki/Gibbs_sampling>`_.\n",
    "    This function parallelizes samplings across bins.\n",
    "    :param numpy.ndarray theta:\n",
    "        parameters used for sampling for each time bin\n",
    "    :param int N:\n",
    "        Number of cells\n",
    "    :param int O:\n",
    "        Order of interaction\n",
    "    :param int R:\n",
    "        Number of runs that are generated.\n",
    "    :returns:\n",
    "        Binary matrix with dimensions (time, runs, cells), in which a `1' in\n",
    "        location (t, r, c) denotes a spike at time t in run r by cell c, as a\n",
    "        numpy.ndarray\n",
    "    \"\"\"\n",
    "    # Set numpy seed (should be removed at some point)\n",
    "    seed = kwargs.get('seed', None)\n",
    "    numpy.random.seed(seed)\n",
    "    # Set pre-trials\n",
    "    pre_R = kwargs.get('pre_n', 100)\n",
    "    # Sample Step\n",
    "    steps = kwargs.get('sample_steps', 1)\n",
    "    # Sample Step\n",
    "    num_proc = kwargs.get('num_proc', 1)\n",
    "    # Get number of bins\n",
    "    T = theta.shape[0]\n",
    "    # Initialize array for spike data\n",
    "    X = numpy.zeros([T, R+pre_R, N], dtype=numpy.uint8)\n",
    "    # Gets subsets\n",
    "    subsets = transforms.enumerate_subsets(N, O)\n",
    "    # Get number of natural parameters\n",
    "    D = len(subsets)\n",
    "    # Initialize subset map\n",
    "    subset_map = numpy.zeros([D, N])\n",
    "    # Get map of relevant patterns (d,c)\n",
    "    for i in range(len(subsets)):\n",
    "        subset_map[i, subsets[i]] = 1\n",
    "    # Count how many cells must be active for each theta\n",
    "    subset_count = numpy.sum(subset_map, axis=1)\n",
    "    # Draw random numbers from uniform distribution\n",
    "    rand_numbers = numpy.random.rand(T, R+pre_R, N)\n",
    "    # Parallel samplings at all time bins\n",
    "    pool = Pool(num_proc)\n",
    "    results = pool.map(partial(gibbs_sampler, X=X, theta=theta, N=N, R=R,\n",
    "        pre_R=pre_R, subset_map=subset_map, subset_count=subset_count, steps=steps), range(T))\n",
    "    pool.close()\n",
    "\n",
    "    return numpy.array(results)\n",
    "\n",
    "\n",
    "def gibbs_sampler(t, X, theta, N, R, pre_R, subset_map, subset_count, steps):\n",
    "    \"\"\" Samples the spike data using Gibbs sampling for time bin t.\n",
    "    \"\"\"\n",
    "    cur_theta = theta[t]\n",
    "    cur_X = numpy.zeros([R*steps+pre_R, N])\n",
    "    numpy.random.seed()\n",
    "    rand_numbers = numpy.random.rand(R*steps + pre_R, N)\n",
    "\n",
    "    for l in range(1, R*steps + pre_R):\n",
    "        # Iterate through all cells\n",
    "        for i in range(N):\n",
    "            # Construct pattern from trial before and\n",
    "            # from neurons that have been seen in this trial\n",
    "            pattern = numpy.hstack([cur_X[l, :i], cur_X[l-1, i:]])\n",
    "            # set x^(i,t) to \"1\" and compute f(X) for those\n",
    "            pattern[i] = 1\n",
    "            fx1 = (numpy.dot(pattern, subset_map.T) == subset_count)\n",
    "            # Set x^(i,t) to \"0\" and compute f(X) for those\n",
    "            pattern[i] = 0\n",
    "            fx0 = (numpy.dot(pattern, subset_map.T) == subset_count)\n",
    "            # compute p( x^(i,l) = 1 || X^(1:i-1,t),X^(i+1:N,l-1) )\n",
    "            prob_spike = 0.5*(1 + numpy.tanh(0.5*(numpy.dot(cur_theta,fx1)\n",
    "                                                - numpy.dot(cur_theta,fx0))))\n",
    "            # if smaller than probability X^(i,l) -> 1\n",
    "            cur_X[l, i] = numpy.greater_equal(prob_spike, rand_numbers[l, i])\n",
    "\n",
    "    return cur_X[pre_R::steps, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
