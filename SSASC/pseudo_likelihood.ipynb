{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains all methods that are concerned with the pseudolikelood\n",
    "approximation.\n",
    "---\n",
    "This code implements approximate inference methods for State-Space Analysis of\n",
    "Spike Correlations (Shimazaki et al. PLoS Comp Bio 2012). It is an extension of\n",
    "the existing code from repository <https://github.com/tomxsharp/ssll> (For\n",
    "Matlab Code refer to <http://github.com/shimazaki/dynamic_corr>). We\n",
    "acknowledge Thomas Sharp for providing the code for exact inference.\n",
    "In this library are additional methods provided to perform the State-Space\n",
    "Analysis approximately. This includes pseudolikelihood, TAP, and Bethe\n",
    "approximations. For details see: <http://arxiv.org/abs/1607.08840>\n",
    "Copyright (C) 2016\n",
    "Authors of the extensions: Christian Donner (christian.donner@bccn-berlin.de)\n",
    "                           Hideaki Shimazaki (shimazaki@brain.riken.jp)\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "import numpy\n",
    "from scipy import sparse\n",
    "\n",
    "import import_ipynb\n",
    "import max_posterior\n",
    "import transforms\n",
    "import mean_field\n",
    "import bethe_approximation\n",
    "\n",
    "\n",
    "MAX_GA_ITERATIONS = 5000\n",
    "Fx_s = None\n",
    "time_bin = -1\n",
    "\n",
    "\n",
    "def compute_Fx_s(X, O):\n",
    "    \"\"\"\n",
    "    Constructs F(x_s=1, x_\\s), feature vectors of interactions up to the\n",
    "    'O'th order from observed patterns for conditional likelihood model.\n",
    "    :param numpy.array X:\n",
    "        Two dimensional (r, c) binary array, where the first dimension is runs\n",
    "        (trials) and the second is the number of cells.\n",
    "    :param int O:\n",
    "        Order of interactions.\n",
    "    :returns Fx_s:\n",
    "        (r, D) sparse matrix, where D is the model dimension.\n",
    "    \"\"\"\n",
    "    T, R, N = X.shape\n",
    "    # Initialize Fx_s\n",
    "    global Fx_s\n",
    "    # List of lists (for each time bin) of sparse matrices (for each cell)\n",
    "    Fx_s = []\n",
    "    # For each time bin\n",
    "    for i in range(T):\n",
    "        # Initialize list\n",
    "        Fx_s.append([])\n",
    "        # Get spike data\n",
    "        # For each cell\n",
    "        for s in range(N):\n",
    "            # Get spike data\n",
    "            Xtmp = X[i,:,:].copy()\n",
    "            # Set current cell to 1\n",
    "            Xtmp[:,s] = 1\n",
    "            # Compute Fx with cell active\n",
    "            Fx1 = compute_Fx(Xtmp, O)\n",
    "            # Get spike data again\n",
    "            Xtmp = X[i,:,:].copy()\n",
    "            # Sett current cell to 0\n",
    "            Xtmp[:,s] = 0\n",
    "            # Compute Fx for cell inactive\n",
    "            Fx2 = compute_Fx(Xtmp, O)\n",
    "            # Create sparse matrix of difference in active and inactive Fx\n",
    "            Fx_s[i].append(sparse.coo_matrix(Fx1 - Fx2))\n",
    "\n",
    "\n",
    "def compute_Fx(X, O):\n",
    "    \"\"\"\n",
    "    Construct feature vectors of interactions up to the 'O'th order from\n",
    "    pattern data.\n",
    "    :param numpy.array X:\n",
    "        (r, c) binary array, where the first dimension are runs (trials)\n",
    "        and second cells.\n",
    "    :param int O:\n",
    "        Order of interactions\n",
    "    :returns Fx:\n",
    "        (r, D) matrix of feature vectors, where D is the model\n",
    "        dimension.\n",
    "    \"\"\"\n",
    "    # Get spike-matrix metadata\n",
    "    R, N = X.shape\n",
    "    # Compute each n-choose-k subset of cell IDs up to the 'O'th order\n",
    "    subsets = transforms.enumerate_subsets(N, O)\n",
    "    # Set up the output array\n",
    "    Fx = numpy.zeros((len(subsets),R))\n",
    "    # Iterate over each subset\n",
    "    for i in range(len(subsets)):\n",
    "        # Select the cells that are in the subset\n",
    "        sp = X[:,subsets[i]]\n",
    "        # Find the timesteps in which all subset-cells spike coincidentally\n",
    "        spc = sp.sum(axis=1) == len(subsets[i])\n",
    "        # Save the observed spike pattern\n",
    "        Fx[i,:] = spc\n",
    "\n",
    "    return Fx\n",
    "\n",
    "\n",
    "def pseudo_newton(y_t, X_t, R, theta_0, theta_o, sigma_o, sigma_o_i,\n",
    "                  param_est_eta='bethe_hybrid'):\n",
    "    \"\"\" Newton-Raphson method with pseudo-log-likelihood as objective function.\n",
    "    :param numpy.ndarray X:\n",
    "        Two dimensional (r, c) binary array, where the first dimension is runs\n",
    "        (trials) and the second is the number of cells.\n",
    "    :param int R:\n",
    "        Number of runs\n",
    "    :param numpy.ndarray theta_0:\n",
    "        Starting point for theta\n",
    "    :param numpy.ndarray theta_o:\n",
    "        One-step prediction for theta\n",
    "    :param sigma_o:\n",
    "        One-step prediction covariance matrix\n",
    "    :param sigma_o_i:\n",
    "        Inverse one-step prediction covariance matrix\n",
    "    :returns:\n",
    "        Tuple containing the mean and covariance of the posterior probability\n",
    "        density, each as a numpy.ndarray.\n",
    "    @author: Christian Donner\n",
    "    \"\"\"\n",
    "    # Read out number of cells and natural parameters\n",
    "    N, D = X_t.shape[1], theta_0.shape[0]\n",
    "    # Initialize theta, iteration counter and maximal derivative of posterior\n",
    "    theta_max = theta_0\n",
    "    iterations = 0\n",
    "    max_dlpo = numpy.Inf\n",
    "    # Intialize array for sum of active thetas (r,c)\n",
    "    fs = numpy.empty([R, N])\n",
    "\n",
    "    # Iterate until convergence or failure\n",
    "    while max_dlpo > max_posterior.GA_CONVERGENCE:\n",
    "\n",
    "        # Initialize gradient and Hessian arrays\n",
    "        dllk = numpy.zeros(D)\n",
    "        ddllk = numpy.zeros([D,D])\n",
    "\n",
    "        # Iterate over all cells\n",
    "        for s_i in range(N):\n",
    "            # Calculate sum of active thetas\n",
    "            fs[:, s_i] = Fx_s[time_bin][s_i].T.dot(theta_max)\n",
    "            # Calculate conditional rate\n",
    "            try:\n",
    "                calc = numpy.less_equal(fs[:,s_i], 709)\n",
    "            except FloatingPointError:\n",
    "                print(numpy.amax(fs))\n",
    "            etas = numpy.ones(fs.shape[0])\n",
    "            etas[calc] = numpy.exp(fs[calc,s_i])/(1.+numpy.exp(fs[calc,s_i]))\n",
    "            # Calculate derivative of conditional rate\n",
    "            deta = - etas * (1-etas)\n",
    "            # Calculate derivative for neuron\n",
    "            dllk += Fx_s[time_bin][s_i].dot(X_t[:, s_i] - etas)\n",
    "            # Fill in detas in Fx_s\n",
    "            Fx_s_deta = sparse.coo_matrix(((deta)[Fx_s[time_bin][s_i].col],\n",
    "                                  [Fx_s[time_bin][s_i].col,\n",
    "                                   Fx_s[time_bin][s_i].row]),\n",
    "                                  [Fx_s[time_bin][s_i].shape[1],\n",
    "                                   Fx_s[time_bin][s_i].shape[0]])\n",
    "            # Compute finally Hesian for Neuron\n",
    "            ddllk += Fx_s[time_bin][s_i].dot(Fx_s_deta)\n",
    "        # Calculate prior\n",
    "        dlpr = -numpy.dot(sigma_o_i, theta_max - theta_o)\n",
    "        # Calculate posterior\n",
    "        dlpo = numpy.array(dllk + dlpr)\n",
    "        # Calculate the Hessian of posterior\n",
    "        ddlpo = numpy.array(ddllk - sigma_o_i)\n",
    "        # Compute the inverse\n",
    "        ddlpo_inv = numpy.linalg.inv(ddlpo)\n",
    "        # Update theta\n",
    "        theta_max = theta_max - 0.1*numpy.dot(ddlpo_inv, dlpo)\n",
    "        # Get maximal entry in gradient and count iteration\n",
    "        max_dlpo = numpy.amax(numpy.absolute(dlpo)) / R\n",
    "        iterations += 1\n",
    "        # Throw Exception if did not converge\n",
    "        if iterations == MAX_GA_ITERATIONS:\n",
    "            raise Exception('The maximum-a-posterior pseudo newton '+\\\n",
    "                'algorithm did not converge before reaching the maximum '+\\\n",
    "                'number iterations.')\n",
    "\n",
    "    # Return fitted theta and Fisher Info matrix\n",
    "    eta = compute_eta[param_est_eta](theta_max, N)\n",
    "    ddllk = -R*bethe_approximation.construct_fisher_diag(eta, N)\n",
    "    #ddllk = pseudo_ddllk(etas,D)\n",
    "    ddlpo = ddllk - sigma_o_i\n",
    "    # Calculate Inverse\n",
    "    ### ddlpo_i = 1./ddlpo#numpy.linalg.inv(ddlpo)\n",
    "    ddlpo_i = 1./ddlpo\n",
    "    return theta_max, -ddlpo_i\n",
    "\n",
    "\n",
    "def pseudo_cg(y_t, X_t, R, theta_0, theta_o, sigma_o, sigma_o_i,\n",
    "              param_est_eta='bethe_hybrid'):\n",
    "    \"\"\" Fits due to non linear conjugate gradient, where Pseudolikelihood is the\n",
    "     objective function.\n",
    "    :param numpy.ndarray X:\n",
    "        Two dimensional (r, c) binary array, where the first dimension is runs\n",
    "        (trials) and the second is the number of cells.\n",
    "    :param int R:\n",
    "        Number of runs\n",
    "    :param numpy.ndarray theta_0:\n",
    "        Starting point for theta\n",
    "    :param numpy.ndarray theta_o:\n",
    "        One-step prediction for theta\n",
    "    :param sigma_o:\n",
    "        One-step prediction covariance matrix\n",
    "    :param sigma_o_i:\n",
    "        Inverse one-step prediction covariance matrix\n",
    "    :returns:\n",
    "        Tuple containing the mean and covariance of the posterior probability\n",
    "        density, each as a numpy.ndarray.\n",
    "    @author: Christian Donner\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract parameters\n",
    "    R, N = X_t.shape\n",
    "    D = theta_0.shape[0]\n",
    "    # Initialize theta\n",
    "    theta_max = theta_0\n",
    "    # Calculate fs = sum(theta_I*F_I(x_s = 1, x_/s))\n",
    "    fs = numpy.empty([R, N])\n",
    "    for s_i in range(N):\n",
    "        fs[:, s_i] = Fx_s[time_bin][s_i].T.dot(theta_max)\n",
    "\n",
    "    # Initialize stopping criterion variables\n",
    "    max_dlpo = numpy.Inf\n",
    "    iterations = 0\n",
    "    # Get likelihood gradient\n",
    "    dllk, etas = pseudo_dllk(theta_max, X_t, fs)\n",
    "    # Get prior\n",
    "    dlpr = -sigma_o_i*(theta_max - theta_o)\n",
    "    # Get posterior\n",
    "    dlpo = dllk + dlpr\n",
    "    # Initialize theta gradient\n",
    "    d_th = dlpo\n",
    "    # Set initial search direction\n",
    "    s = dlpo\n",
    "    # Perform first line search\n",
    "    theta_max, fs = pseudo_line_search2(theta_max, X_t, s, fs, dlpo, sigma_o_i,\n",
    "                                       etas, theta_o)\n",
    "    # Calculate new likelihood gradient\n",
    "    dllk, etas = pseudo_dllk(theta_max, X_t, fs)\n",
    "    # and new prior\n",
    "    dlpr = -sigma_o_i*(theta_max - theta_o)\n",
    "    # and new Posterior\n",
    "    dlpo = dllk + dlpr\n",
    "\n",
    "    # Iterate until convergence or failure\n",
    "    while max_dlpo > max_posterior.GA_CONVERGENCE:\n",
    "        # Set old theta direction\n",
    "        d_th_prev = d_th\n",
    "        # Set posterior to new theta direction\n",
    "        d_th = dlpo\n",
    "        # Calculate beta\n",
    "        beta = max_posterior.compute_beta(d_th, d_th_prev, 'HS')\n",
    "        # Set new search direction\n",
    "        s = d_th + beta * s\n",
    "        # Perform line search in this direction\n",
    "        theta_max, fs = pseudo_line_search2(theta_max, X_t, s, fs, dlpo,\n",
    "                                            sigma_o_i, etas, theta_o)\n",
    "        # Calculate the new gradient and conditional rates\n",
    "        dllk, etas = pseudo_dllk(theta_max, X_t, fs)\n",
    "\n",
    "        # Calculate prior\n",
    "        dlpr = -sigma_o_i*(theta_max - theta_o)\n",
    "        # Calculate posterior\n",
    "        dlpo = dllk + dlpr\n",
    "        # Get maximal entry of posterior gradient an count iterations\n",
    "        max_dlpo = numpy.amax(numpy.absolute(dlpo)) / R\n",
    "        iterations += 1\n",
    "        # Throw exceptio if not converged\n",
    "        if iterations == MAX_GA_ITERATIONS:\n",
    "            raise Exception('The pseudo conjugate gradient '+\\\n",
    "                'algorithm did not converge before reaching the maximum '+\\\n",
    "                'number iterations.')\n",
    "\n",
    "    # Compute final Hessian of posterior\n",
    "    #eta = mean_field.forward_problem_hessian(theta_max, N, 'TAP')\n",
    "    eta = compute_eta[param_est_eta](theta_max, N)\n",
    "    ddllk = -R*bethe_approximation.construct_fisher_diag(eta, N)\n",
    "    #ddllk = pseudo_ddllk(etas,D)\n",
    "    ddlpo = ddllk - sigma_o_i\n",
    "    # Calculate Inverse\n",
    "    ### ddlpo_i = 1./ddlpo#numpy.linalg.inv(ddlpo)\n",
    "    ddlpo_i = 1./ddlpo\n",
    "    # Return fitted theta and Fisher Info matrix\n",
    "    return theta_max, -ddlpo_i\n",
    "\n",
    "\n",
    "def pseudo_bfgs(y_t, X_t, R, theta_0, theta_o, sigma_o, sigma_o_i,\n",
    "                param_est_eta='bethe_hybrid'):\n",
    "    \"\"\" Fits due to Broyden-Fletcher-Goldfarb-Shanno algorithm, where\n",
    "    Pseudolikelihood is the objective function.\n",
    "    :param numpy.ndarray X:\n",
    "        Two dimensional (r, c) binary array, where the first dimension is runs\n",
    "        (trials) and the second is the number of cells.\n",
    "    :param int R:\n",
    "        Number of runs\n",
    "    :param numpy.ndarray theta_0:\n",
    "        Starting point for theta\n",
    "    :param numpy.ndarray theta_o:\n",
    "        One-step prediction for theta\n",
    "    :param sigma_o:\n",
    "        One-step prediction covariance matrix\n",
    "    :param sigma_o_i:\n",
    "        Inverse one-step prediction covariance matrix\n",
    "    :returns:\n",
    "        Tuple containing the mean and covariance of the posterior probability\n",
    "        density, each as a numpy.ndarray.\n",
    "    @author: Christian Donner\n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of cells and natural parameters\n",
    "    N, D = X_t.shape[1], theta_0.shape[0]\n",
    "    # Initialize theta with previous smoothed theta\n",
    "    theta_max = theta_0\n",
    "    # Calculate fs = sum(theta_I*F_I(x_s = 1, x_/s))\n",
    "    fs = numpy.empty([R, N])\n",
    "    for s_i in range(N):\n",
    "        fs[:, s_i] = Fx_s[time_bin][s_i].T.dot(theta_max)\n",
    "\n",
    "    # Initialize the estimate of the inverse fisher info\n",
    "    ddlpo_i_e = numpy.identity(theta_max.shape[0])\n",
    "    # Initialize stopping criterion variables\n",
    "    max_dlpo = 1.\n",
    "    iterations = 0\n",
    "    # Compute derivative of posterior\n",
    "    dllk, etas = pseudo_dllk(theta_max, X_t, fs)\n",
    "    dlpr = -sigma_o_i*(theta_max - theta_o)\n",
    "    dlpo = dllk + dlpr\n",
    "    # Iterate until convergence or failure\n",
    "    while max_dlpo > max_posterior.GA_CONVERGENCE:\n",
    "\n",
    "        # Compute direction for line search\n",
    "        s_dir = numpy.dot(dlpo, ddlpo_i_e)\n",
    "        # Set theta to old theta\n",
    "        theta_prev = numpy.copy(theta_max)\n",
    "        # Set current log posterior gradient to previous\n",
    "        dlpo_prev = dlpo\n",
    "        # Perform line search\n",
    "        theta_max, fs = pseudo_line_search2(theta_max, X_t, s_dir, fs, dlpo,\n",
    "                                           sigma_o_i, etas, theta_o)\n",
    "        # Get the difference between old and new theta\n",
    "        d_theta = theta_max - theta_prev\n",
    "        # Compute derivative of posterior\n",
    "        dllk, etas = pseudo_dllk(theta_max, X_t, fs)\n",
    "        dlpr = -sigma_o_i*(theta_max - theta_o)\n",
    "        dlpo = dllk + dlpr\n",
    "        # Difference in log posterior gradients\n",
    "        dlpo_diff = dlpo_prev - dlpo\n",
    "        # Project gradient change on theta change\n",
    "        dlpo_diff_dth = numpy.inner(dlpo_diff, d_theta)\n",
    "        # Compute estimate of covariance matrix with Sherman-Morrison Formula\n",
    "        a = (dlpo_diff_dth + \\\n",
    "             numpy.dot(dlpo_diff, numpy.dot(ddlpo_i_e, dlpo_diff.T)))*\\\n",
    "            numpy.outer(d_theta, d_theta)\n",
    "        b = numpy.inner(d_theta, dlpo_diff)**2\n",
    "        c = numpy.dot(ddlpo_i_e, numpy.outer(dlpo_diff, d_theta)) + \\\n",
    "            numpy.outer(d_theta, numpy.inner(dlpo_diff, ddlpo_i_e))\n",
    "        d = dlpo_diff_dth\n",
    "        ddlpo_i_e += (a/b - c/d)\n",
    "        # Get maximal entry of log posterior grad divided by number of trials\n",
    "        max_dlpo = numpy.amax(numpy.absolute(dlpo)) / R\n",
    "        # Count iterations\n",
    "        iterations += 1\n",
    "        if iterations == MAX_GA_ITERATIONS:\n",
    "            raise Exception('The pseudo bfgs '+\\\n",
    "                'algorithm did not converge before reaching the maximum '+\\\n",
    "                'number iterations.')\n",
    "\n",
    "    # Return fitted theta and Fisher Info matrix\n",
    "    eta = compute_eta[param_est_eta](theta_max, N)\n",
    "    ddllk = -R*bethe_approximation.construct_fisher_diag(eta, N)\n",
    "    #ddllk = pseudo_ddllk(etas,D)\n",
    "    ddlpo = ddllk - sigma_o_i\n",
    "    # Calculate Inverse\n",
    "    ### ddlpo_i = 1./ddlpo#numpy.linalg.inv(ddlpo)\n",
    "    ddlpo_i = 1./ddlpo\n",
    "    return theta_max, -ddlpo_i\n",
    "\n",
    "\n",
    "def pseudo_line_search(theta, X, s, fs, dlpo, sigma_o_i, etas):\n",
    "    \"\"\" Performs the line search for pseudo-log-likelihood as objective\n",
    "    function by quadratic approximation at current theta.\n",
    "    :param numpy.ndarray theta:\n",
    "        (d,) natural parameters\n",
    "    :param numpy.ndarray X:\n",
    "        (r,c) spike data\n",
    "    :param numpy.ndarray s:\n",
    "        (d,) search direction\n",
    "    :param numpy.ndarray fs:\n",
    "        (r,c) sum of active thetas for run and cell\n",
    "    :param numpy.ndarray dlpo:\n",
    "        (d,) derivative of posterior\n",
    "    :param numpy.ndarray:\n",
    "        (d,d) inverse of one-step covariance\n",
    "    :param numpy.ndarray etas:\n",
    "        (r,c) conditional rate for each run and cell\n",
    "    :returns:\n",
    "        (d,) new theta according to quadratic approximation\n",
    "        (r, c) new sums of active thetas\n",
    "    \"\"\"\n",
    "    # Extract number of runs and cells\n",
    "    R, N = X.shape\n",
    "    # Initialize array for Fx_s projection on search direction (r,c)\n",
    "    Fx_s_s = numpy.empty([R, N])\n",
    "    # Iterate of all cells and project Fx_s on search direction\n",
    "    for s_i in range(N):\n",
    "        Fx_s_s[:, s_i] = Fx_s[time_bin][s_i].T.dot(s)\n",
    "    # Project posterior on search direction\n",
    "    dlpo_s = numpy.dot(dlpo.T, s)\n",
    "    # Project conditional rate on search direction\n",
    "    detas = etas*(1-etas)\n",
    "    # Project one-step covariance matrix on search direction\n",
    "    sigma_o_i_s = numpy.dot(s, numpy.dot(sigma_o_i, s))\n",
    "    # Compute projection of pseudo-log-likelihood Hessian on search direction\n",
    "    ddlpo_s = numpy.tensordot(detas*Fx_s_s, Fx_s_s, ((1,0),(1,0))) + sigma_o_i_s\n",
    "    # Compute how much the step should be along search direction\n",
    "    alpha = dlpo_s/ddlpo_s\n",
    "    # Update sum of active thetas\n",
    "    fs_new = fs + alpha*Fx_s_s\n",
    "    # Update theta\n",
    "    theta_new = theta + alpha*s\n",
    "    # Return\n",
    "    return theta_new, fs_new\n",
    "\n",
    "\n",
    "def pseudo_line_search2(theta, X, s, fs, dlpo, sigma_o_i_tmp, etas, theta_o):\n",
    "    \"\"\" Performs the line search for pseudo-log-likelihood as objective\n",
    "    function by quadratic approximation at current theta, but does more than one\n",
    "    step.\n",
    "    :param numpy.ndarray theta:\n",
    "        (d,) natural parameters\n",
    "    :param numpy.ndarray X:\n",
    "        (r,c) spike data\n",
    "    :param numpy.ndarray s:\n",
    "        (d,) search direction\n",
    "    :param numpy.ndarray fs:\n",
    "        (r,c) sum of active thetas for run and cell\n",
    "    :param numpy.ndarray dlpo:\n",
    "        (d,) derivative of posterior\n",
    "    :param numpy.ndarray:\n",
    "        (d,d) inverse of one-step covariance\n",
    "    :param numpy.ndarray etas:\n",
    "        (r,c) conditional rate for each run and cell\n",
    "    :returns:\n",
    "        (d,) new theta according to quadratic approximation\n",
    "        (r, c) new sums of active thetas\n",
    "    \"\"\"\n",
    "    # Extract number of runs and cells\n",
    "    R, N = X.shape\n",
    "    sigma_o_i = numpy.diag(sigma_o_i_tmp)\n",
    "    # Initialize array for Fx_s projection on search direction (r,c)\n",
    "    Fx_s_s = numpy.empty([R, N])\n",
    "    # Iterate of all cells and project Fx_s on search direction\n",
    "    for s_i in range(N):\n",
    "        Fx_s_s[:, s_i] = Fx_s[time_bin][s_i].T.dot(s)\n",
    "    # Project posterior on search direction\n",
    "    dlpo_s = numpy.dot(dlpo.T, s)\n",
    "    num_iter = 0\n",
    "    conv = numpy.inf\n",
    "    while conv > 1e-2 and num_iter < 10:\n",
    "        dlpo_s_old = numpy.absolute(dlpo_s)\n",
    "        # Project conditional rate on search direction\n",
    "        detas = etas*(1-etas)\n",
    "        # Project one-step covariance matrix on search direction\n",
    "        sigma_o_i_s = numpy.dot(s, numpy.dot(sigma_o_i, s))\n",
    "        # Compute projection of pseudologlikelihood Hessian on search direction\n",
    "        ddlpo_s = numpy.tensordot(detas*Fx_s_s, Fx_s_s, ((1,0),(1,0))) +\\\n",
    "                  sigma_o_i_s\n",
    "        # Compute how much the step should be along search direction\n",
    "        alpha = dlpo_s/ddlpo_s\n",
    "        # Update sum of active thetas\n",
    "        fs_new = fs + .5*alpha*Fx_s_s\n",
    "        # Update theta\n",
    "        theta_new = theta + .5*alpha*s\n",
    "        dllk, etas = pseudo_dllk(theta_new, X, fs)\n",
    "        # Calculate prior\n",
    "        dlpr = -numpy.dot(sigma_o_i, theta_new - theta_o)\n",
    "        dlpo = dllk + dlpr\n",
    "        dlpo_s = numpy.dot(dlpo.T, s)\n",
    "        conv = numpy.absolute(dlpo_s_old-dlpo_s)\n",
    "        num_iter += 1\n",
    "    # Return\n",
    "    return theta_new, fs_new\n",
    "\n",
    "\n",
    "def compute_cond_eta(theta, t):\n",
    "    \"\"\" Computes conitional rate\n",
    "    :param numpy.ndarray theta:\n",
    "        (d) array with thetas at time t\n",
    "    :param int t:\n",
    "        time index of theta\n",
    "    :returns:\n",
    "        (N,) array whit conditional rates for each neuron\n",
    "    \"\"\"\n",
    "    N = len(Fx_s[t])\n",
    "    R = Fx_s[t][0].shape[1]\n",
    "    fs = numpy.empty([R, N])\n",
    "    for s_i in range(N):\n",
    "        fs[:, s_i] = Fx_s[t][s_i].T.dot(theta)\n",
    "    try:\n",
    "        calc = numpy.less_equal(fs, 709)\n",
    "    except FloatingPointError:\n",
    "        print(numpy.amax(fs))\n",
    "    etas = numpy.ones(fs.shape)\n",
    "    etas[calc] = numpy.exp(fs[calc])/(1.+numpy.exp(fs[calc]))\n",
    "    return numpy.mean(etas, axis=0)\n",
    "\n",
    "\n",
    "def pseudo_dllk(theta, X, fs):\n",
    "    \"\"\" Calculates the gradient of the pseudo-log-likelihood.\n",
    "    :param numpy.ndarray theta:\n",
    "        (d,) array of natural parameters\n",
    "    :param numpy.ndarray X:\n",
    "        (r,c) array with spike data\n",
    "    :param numpy.ndarray fs:\n",
    "        (r,c) array containing sum of 'active thetas' for data\n",
    "    :returns:\n",
    "        (d,) numpy.ndarray with gradient\n",
    "        (r,c) numpy.ndarray with conditional rates\n",
    "    \"\"\"\n",
    "    # Get number of cells\n",
    "    N = X.shape[1]\n",
    "    # Initialize gradient array\n",
    "    dllk = numpy.zeros(theta.shape[0])\n",
    "    # Calculate conditional rate\n",
    "    calc = numpy.less_equal(fs, 709)\n",
    "    etas = numpy.ones(fs.shape)\n",
    "    etas[calc] = numpy.exp(fs[calc])/(1.+numpy.exp(fs[calc]))\n",
    "    # Iterate over all cells\n",
    "    for s_i in range(N):\n",
    "        # Add gradient for each cell\n",
    "        dllk += Fx_s[time_bin][s_i].dot((X[:,s_i] - etas[:,s_i]))\n",
    "    # Return\n",
    "    return dllk, etas\n",
    "\n",
    "\n",
    "def pseudo_ddllk(etas, D):\n",
    "    \"\"\" Calculates the Hessian for the pseudo-log-likelihood.\n",
    "    :param numpy.ndarray etas:\n",
    "        (r,c) array of conditional rate\n",
    "    :param int D:\n",
    "        number of natural parameters\n",
    "    :returns\n",
    "        (d,d) array with Hessian of pseudo-log-likelihood\n",
    "    \"\"\"\n",
    "    # Get number of cells\n",
    "    N = etas.shape[1]\n",
    "    # Intitialize Hessian\n",
    "    ddllk = numpy.zeros([D,D])\n",
    "    # iteratate over all cells\n",
    "    for s_i in range(N):\n",
    "        # Calculate the derivative of conditional rate wrt. theta\n",
    "        deta = -etas[:, s_i]*(1-etas[:, s_i])\n",
    "        # Fill the derivatives where Fx_s one\n",
    "        Fx_s_deta = sparse.coo_matrix(((deta)[Fx_s[time_bin][s_i].col],\n",
    "                                [Fx_s[time_bin][s_i].col,\n",
    "                                 Fx_s[time_bin][s_i].row]),\n",
    "                                [Fx_s[time_bin][s_i].shape[1],\n",
    "                                 Fx_s[time_bin][s_i].shape[0]])\n",
    "        # Compute final Hessian for each cell\n",
    "        ddllk += Fx_s[time_bin][s_i].dot(Fx_s_deta)\n",
    "    # Return\n",
    "    return ddllk\n",
    "\n",
    "\n",
    "def pseudo_log_likelihood(X_t, theta, t):\n",
    "    \"\"\" Computes the pseudo-log-likelihood for data and theta\n",
    "    :param numpy.ndarray X_t:\n",
    "        (r,c) array containing spike data\n",
    "    :param numpy.ndarray theta:\n",
    "        (d) array containing natural parameters\n",
    "    :param int t:\n",
    "        time bin of data and theta\n",
    "    :returns float:\n",
    "        pseudo-log-likelihood\n",
    "    \"\"\"\n",
    "    # Extraxt trial and Cell number\n",
    "    R, N = X_t.shape\n",
    "    # Initialize pseudo-log-likelihood\n",
    "    pseudo_llk = 0\n",
    "    # Run over all cells\n",
    "    for s_i in range(N):\n",
    "        # Calculate fs\n",
    "        fs = Fx_s[t][s_i].T.dot(theta)\n",
    "        # and pseudo-log-likelihood for each cell\n",
    "        pseudo_llk += numpy.sum(X_t[:,s_i]*fs - numpy.log(1 + numpy.exp(fs)))\n",
    "    # Return\n",
    "    return pseudo_llk\n",
    "\n",
    "\n",
    "functions = {'nr': pseudo_newton,\n",
    "             'cg': pseudo_cg,\n",
    "             'bf': pseudo_bfgs}\n",
    "\n",
    "compute_eta = {'mf': mean_field.forward_problem_hessian,\n",
    "               'bethe_BP': bethe_approximation.compute_eta_BP,\n",
    "               'bethe_CCCP': bethe_approximation.compute_eta_CCCP,\n",
    "               'bethe_hybrid': bethe_approximation.compute_eta_hybrid}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
